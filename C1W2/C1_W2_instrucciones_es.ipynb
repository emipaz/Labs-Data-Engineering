{"cells": [{"cell_type": "markdown", "id": "fa364016-7c0c-4284-9ca4-bb7cf1c52e49", "metadata": {}, "source": ["# Tarea de la Semana 2:<br>Un Ejemplo del Ciclo de Vida de la Ingenier\u00eda de Datos\n"]}, {"cell_type": "markdown", "id": "c6b29218-9092-4b5f-a371-726f2225d064", "metadata": {}, "source": ["En este laboratorio, configurar\u00e1s y ejecutar\u00e1s un ejemplo de canalizaci\u00f3n de datos que muestra todas las etapas del ciclo de vida de la ingenier\u00eda de datos. Tu sistema fuente ser\u00e1 una base de datos relacional instanciada como base de datos MySQL en Amazon RDS (Servicio de Base de Datos Relacional). Primero explorar\u00e1s la base de datos fuente con un conjunto de datos de muestra, luego usar\u00e1s AWS Glue para extraer, transformar y cargar (ETL) los datos en tu canalizaci\u00f3n de datos, almacen\u00e1ndolos en el almacenamiento de objetos de AWS Amazon S3. Finalmente, consultar\u00e1s los datos almacenados utilizando Amazon Athena para construir un panel de visualizaci\u00f3n de datos en Jupyter Lab. Para definir y configurar los componentes de este ejemplo de canalizaci\u00f3n de datos, utilizar\u00e1s Terraform como servicio de Infraestructura como C\u00f3digo (IaC).\n"]}, {"cell_type": "markdown", "id": "56183f36-43f9-4e48-9661-eb999b1417f4", "metadata": {}, "source": ["# Tabla de Contenidos\n", "- [ 1 - Introducci\u00f3n](#1)\n", "- [ 2 - Explorando el Sistema Fuente](#2)\n", "- [ 3 - Arquitectura del Pipeline de Datos](#3)\n", "- [ 4 - Extracci\u00f3n, Transformaci\u00f3n y Carga de Datos](#4)\n", "- [ 5 - Datos e Visualizaciones](#5)\n"]}, {"cell_type": "markdown", "id": "a7e9b3cb-4971-4a00-bb6d-54b75b4d0c9c", "metadata": {}, "source": ["<div id='1'/>\n", "\n", "## 1 - Introducci\u00f3n\n"]}, {"cell_type": "markdown", "id": "d5d6f0d9-7813-40aa-81ba-602de0018ecd", "metadata": {}, "source": ["Suponga que trabaja como Ingeniero de Datos en un minorista de modelos a escala de autos cl\u00e1sicos y otros medios de transporte. El minorista almacena sus compras hist\u00f3ricas y la informaci\u00f3n de sus clientes en una base de datos relacional que consta de las siguientes tablas: clientes, productos, l\u00edneas de productos, pedidos, detalles de pedidos, pagos, empleados, oficinas. En este laboratorio, utilizar\u00e1 un ejemplo de dicha base de datos: [Base de datos de muestra de MySQL](https://www.mysqltutorial.org/mysql-sample-database.aspx)\n", "\n", "El analista de datos del equipo de marketing est\u00e1 interesado en analizar las compras hist\u00f3ricas para comprender, por ejemplo, qu\u00e9 l\u00ednea de productos es m\u00e1s exitosa y c\u00f3mo se distribuyen las ventas en los diferentes pa\u00edses. Si bien el analista de datos puede consultar directamente los datos de la base de datos relacional, es posible que necesite escribir consultas complejas que podr\u00edan tardar mucho tiempo en recuperar los datos requeridos. Realizar consultas anal\u00edticas en bases de datos de aplicaciones de producci\u00f3n generalmente es una mala idea, porque podr\u00eda afectar el rendimiento de esa base de datos. Su trabajo como Ingeniero de Datos es construir un canal de datos que transforme los datos en una forma m\u00e1s f\u00e1cil de entender y m\u00e1s r\u00e1pida de consultar, y servirlo al analista de datos para que pueda centrarse \u00fanicamente en los esfuerzos anal\u00edticos.\n", "\n", "En este laboratorio, explorar\u00e1 un ejemplo de extremo a extremo que muestra la implementaci\u00f3n de las etapas del ciclo de vida de la ingenier\u00eda de datos en AWS, y se familiarizar\u00e1 con el entorno del laboratorio.\n", "\n", "\u00a1Comencemos primero explorando el sistema fuente!\n"]}, {"cell_type": "markdown", "id": "09212a04-f22b-4a49-aa83-8d6c70876fc0", "metadata": {}, "source": ["<div id='2'/>\n", "\n", "## 2 - Explorando el Sistema Fuente\n"]}, {"cell_type": "markdown", "id": "8d945b14-f27b-487c-8d36-f34c7979dbce", "metadata": {}, "source": ["Como Ingeniero de Datos, generalmente no mantienes ni creas los sistemas fuente porque est\u00e1n fuera de tu control. Tu trabajo es construir el pipeline de datos que ingiere y combina datos de los sistemas fuente. En este laboratorio, se te proporciona una base de datos MySQL que representa el sistema fuente con el que interactuar\u00e1s. La base de datos proporcionada contiene las tablas que enumeramos en la secci\u00f3n anterior, y se instancia utilizando Amazon RDS, que es un servicio gestionado que te permite ejecutar bases de datos relacionales en la nube. Antes de ejecutar el pipeline de datos, explorar\u00e1s r\u00e1pidamente esta base de datos.\n"]}, {"cell_type": "markdown", "id": "3fbb5e62-de76-4e4a-abb6-6809c37bb7fe", "metadata": {}, "source": ["2.1. Para conectarse a la instancia de Amazon RDS MySQL, necesitas obtener su punto de conexi\u00f3n. \n", "Un punto de conexi\u00f3n es la URL del punto de entrada para el servicio web de AWS. \n", "Obt\u00e9n el punto de conexi\u00f3n de la instancia de la base de datos con el siguiente comando \n", "(reemplaza `<MySQL-DB-name>` con `de-c1w2-rds`).\n", "\n", "Ejecuta el comando en la terminal de VSCode. Si es necesario, abre \n", "una nueva terminal seleccionando Terminal > Nueva terminal en el men\u00fa.\n", "\n", "``` bash\n", "aws rds describe-db-instances --db-instance-identifier <MySQL-DB-name> --output text --query \"DBInstances[].Endpoint.Address\"\n", "```\n"]}, {"cell_type": "markdown", "id": "79052c18-4191-4cc4-921a-6a02c1988355", "metadata": {}, "source": ["2.2. Ahora conectese a la base de datos ejecutando el siguiente comando, reemplazando\n", "- `<MySQLEndpoint>` con la salida del paso anterior,\n", "\n", "- `<DatabaseUserName>` con `admin` ,\n", "\n", "- `<Password>` con `adminpwrd` :\n", "\n", "``` bash\n", "mysql --host=<MySQLEndpoint> --user=<DatabaseUserName> --password=<Password> --port=3306\n", "```\n"]}, {"cell_type": "markdown", "id": "572b924a-e660-4ac6-a7db-593581d9b7db", "metadata": {}, "source": ["2.3. Despu\u00e9s de haber establecido la conexi\u00f3n con la instancia de RDS, puedes verificar la base de datos etiquetada como `classicmodels` y verificar las tablas que contiene ejecutando los siguientes comandos:\n", "\n", "``` bash\n", "use classicmodels;\n", "show tables;\n", "```\n", "\n", "Deber\u00edas obtener una salida similar a esta:\n", "\n", "![show_tables](./images/show_tables.png)\n", "\n", "El script utilizado para poblar la base de datos se puede encontrar en *data/mysqlsampledatabase.sql*. Revisa este archivo e intenta identificar algunos de los comandos. No te preocupes si no entiendes todas las declaraciones; obtendr\u00e1s una mejor comprensi\u00f3n de este lenguaje en futuros laboratorios. Como se mencion\u00f3 anteriormente, la base de datos ya est\u00e1 poblada para ti, por lo que no es necesario que modifiques o ejecutes este archivo.\n"]}, {"cell_type": "markdown", "id": "750f8bbc-e125-4b5c-a85f-d696ea0aba36", "metadata": {}, "source": ["2.4. Ingresa `exit` en la terminal para cerrar la conexi\u00f3n con la base de datos.\n", "\n", "``` bash\n", "exit\n", "```\n"]}, {"cell_type": "markdown", "id": "c93e01a6-95fb-4a6f-aa2c-8e0e0c57e01f", "metadata": {}, "source": ["2.5. Tambi\u00e9n puedes verificar el RDS en la consola de AWS. Para acceder a la consola, ejecuta el siguiente comando en la terminal.\n", "\n", "```bash\n", "cat ../.aws/aws_console_url\n", "```\n", "Abre el enlace en una nueva ventana del navegador. Encuentra **RDS**, haz clic en **Bases de datos** en el panel izquierdo y verifica que la base de datos `de-c1w2-rds` est\u00e9 disponible.\n", "\n", "*Nota*: Por razones de seguridad, la URL para acceder a la consola de AWS caducar\u00e1 cada 15 minutos, \n", "pero cualquier recurso de AWS que hayas creado seguir\u00e1 estando disponible durante el per\u00edodo de 2 horas. \n", "Si necesitas acceder a la consola despu\u00e9s de 15 minutos, vuelve a ejecutar el comando para obtener un nuevo enlace activo.\n", "\n", "*Nota*: Si ves la ventana como en la siguiente captura de pantalla, haz clic en el enlace **cerrar sesi\u00f3n**, \n", "cierra la ventana y vuelve a hacer clic en el enlace de la consola.\n", "\n", "![AWSLogout](images/AWSLogout.png)\n"]}, {"cell_type": "markdown", "id": "a7709fda-aeac-487b-b187-2422fe05a38e", "metadata": {}, "source": ["<div id='3'/>\n", "\n", "## 3 - Arquitectura del Pipeline de Datos\n"]}, {"cell_type": "markdown", "id": "2ce02eef-f31b-4344-b97d-291a9dc7c4bd", "metadata": {}, "source": ["Aqu\u00ed est\u00e1 la arquitectura propuesta del pipeline de datos. Esta arquitectura puede servir como una soluci\u00f3n al problema presentado en este laboratorio. Extrae los datos de la instancia de base de datos RDS MySQL y utiliza AWS Glue para transformar y almacenar los datos en un bucket de S3 (instancia de almacenamiento de objetos), haci\u00e9ndolos consultables para el analista de datos a trav\u00e9s de Amazon Athena.\n", "\n", "![etl_diagram](images/ETL.drawio.png)\n", "\n", "Aqu\u00ed hay una breve descripci\u00f3n de los componentes:\n", "\n", "- **Base de datos de origen:** Ya interactuaste con la base de datos de origen en la secci\u00f3n anterior. Esta es una base de datos relacional que alberga tablas estructuradas y normalizadas. M\u00e1s espec\u00edficamente, representa un sistema de Procesamiento de Transacciones en L\u00ednea (OLTP) que almacena datos transaccionales, donde una transacci\u00f3n en este laboratorio representa una orden de venta realizada por un cliente.\n", "\n", "- **Extracci\u00f3n, Transformaci\u00f3n y Carga (ETL):** El segundo componente es [AWS Glue](https://aws.amazon.com/en/glue/).\n", "\n", "  - **Extracci\u00f3n:** Este paso implica extraer datos de la base de datos OLTP. Se utiliza un trabajo de AWS Glue para conectarse a la base de datos RDS y recuperar datos.\n", "\n", "  - **Transformaci\u00f3n:** Despu\u00e9s de extraer los datos, Glue realiza la transformaci\u00f3n de datos. Puedes especificar a Glue qu\u00e9 tipo de transformaci\u00f3n te gustar\u00eda realizar en los datos extra\u00eddos. Para este laboratorio, la transformaci\u00f3n consiste en modelar los datos en un esquema estrella, que difiere del de la base de datos de origen. El esquema estrella es una forma que hace que los datos sean m\u00e1s legibles y m\u00e1s f\u00e1ciles de usar para el analista de datos, y simplifica la escritura de consultas para extraer los datos necesarios para su an\u00e1lisis. Convertir los datos en un esquema estrella puede incluir tareas como desnormalizaci\u00f3n, agregaciones y cualquier limpieza o enriquecimiento de datos necesario.\n", "\n", "  - **Carga:** Este paso implica almacenar los datos transformados en un sistema de almacenamiento. El sistema de almacenamiento elegido en este laboratorio es la soluci\u00f3n de almacenamiento de objetos de AWS: [Amazon S3](https://aws.amazon.com/en/s3/). S3 es una soluci\u00f3n de almacenamiento escalable y rentable que puede formar parte de sistemas de almacenamiento m\u00e1s abstractos como los data lakes y data warehousing. Los datos transformados se almacenan en un formato llamado Parquet que est\u00e1 optimizado para su uso anal\u00edtico.\n", "\n", "- **Capa de servicio:** [Amazon Athena](https://aws.amazon.com/en/athena/), un servicio de consultas, se utiliza para consultar los datos almacenados en un bucket de S3. Permite realizar consultas similares a SQL sin necesidad de extraer los datos de S3 y cargarlos en una base de datos tradicional. Los usuarios (en este caso el analista de datos) pueden ejecutar consultas ad-hoc con fines anal\u00edticos e informes.\n"]}, {"cell_type": "markdown", "id": "791b1bf2-929a-4d2b-8918-ba854f16903d", "metadata": {}, "source": ["<div id='4'/>\n", "\n", "## 4 - Extrayendo, Transformando y Cargando Datos\n"]}, {"cell_type": "markdown", "id": "3404b9ef-4b75-4e25-a199-c24fd61464f0", "metadata": {}, "source": ["Ahora que tienes una comprensi\u00f3n de los diferentes componentes de tu arquitectura de datos, primero configurar\u00e1s el canal de datos y luego ejecutar\u00e1s las diferentes etapas del mismo. Necesitas declarar y configurar los diferentes componentes de tu canal de datos. Para hacerlo, utilizar\u00e1s [Terraform](https://www.terraform.io/), que es una de las herramientas de Infraestructura como C\u00f3digo (IaC) m\u00e1s populares desarrollada por HashiCorp. Permite a los usuarios definir y aprovisionar infraestructura utilizando un lenguaje de configuraci\u00f3n declarativo (declarativo significa que los usuarios solo necesitan describir los componentes del canal de datos sin instruir sobre los pasos detallados necesarios para construir el canal de datos). Terraform es compatible con proveedores de servicios en la nube y tambi\u00e9n puede gestionar infraestructura local.\n"]}, {"cell_type": "markdown", "id": "5370e4d2-39e1-4f03-9780-26b6f555c863", "metadata": {}, "source": ["4.1. Verifique el contenido del directorio *terraform*. En ese directorio, encontrar\u00e1 algunos archivos con la extensi\u00f3n `tf`. Esos archivos definen las configuraciones de los recursos necesarios para su arquitectura de datos. Los recursos est\u00e1n separados en diferentes scripts solo por legibilidad, lo que significa que no necesariamente encontrar\u00e1 la misma estructura de archivos en todos los proyectos que utilizan Terraform para IaC. Intente tener una visi\u00f3n general de los recursos utilizados, pero no se preocupe si no entiende el c\u00f3digo y todos los componentes en esta etapa.\n"]}, {"cell_type": "markdown", "id": "056bd5d4-f423-4d16-b39d-6c7096e30f67", "metadata": {}, "source": ["4.2. Los archivos `tf` que verificaste te permiten declarar los recursos de tu arquitectura de datos. A\u00fan necesitas especificar para AWS Glue c\u00f3mo realizar la extracci\u00f3n, transformaci\u00f3n y carga de datos. Puedes verificar todos estos pasos como un script de Python [terraform/assets/glue_job.py](terraform/assets/glue_job.py). Nuevamente, no necesitas preocuparte por los detalles. \u00a1Ahora pongamos en acci\u00f3n estos pasos!\n"]}, {"cell_type": "markdown", "id": "df91ee1c-b1df-496b-a45b-06acbb16c500", "metadata": {}, "source": ["4.3. Ejecuta el siguiente comando.\n", "\n", "```bash\n", "source scripts/setup.sh\n", "```\n", "\n", "Los archivos de configuraci\u00f3n de Terraform dependen de algunas variables de entrada que representan el nombre del proyecto, los par\u00e1metros de la base de datos y otros nombres de recursos. El script `setup.sh` contiene las definiciones de estas variables. Una vez que lo ejecutes, obtendr\u00e1 los valores de estas variables y los registrar\u00e1 como variables de entorno con el prefijo `TF_VAR_` en el archivo `~/.bashrc`. Luego, se ejecutar\u00e1 el script `~/.bashrc` para hacer que las variables est\u00e9n disponibles para la configuraci\u00f3n de Terraform.\n"]}, {"cell_type": "markdown", "id": "37f70ea4-9276-4097-bfb2-ce148a8da451", "metadata": {}, "source": ["4.4. En la terminal, navega hasta el directorio de terraform ejecutando el comando:\n", "\n", "``` bash\n", "cd terraform\n", "```\n", "\n", "*Nota*: Si hay errores en los comandos o archivos de configuraci\u00f3n de Terraform, la terminal puede colapsar. \n", "Cuando esto suceda, ver\u00e1s el siguiente mensaje:\n", "\n", "![etl_diagram](images/terminal_crash.png)\n", "\n", "Puedes reabrir la terminal presionando <code>Ctrl + \\`</code> (o <code>Cmd + \\`</code>) o navegando a Ver > Terminal. \n", "En la terminal, vuelve a la carpeta de Terraform (`cd terraform`) y luego intenta \n", "ejecutar nuevamente los comandos requeridos. El error deber\u00eda aparecer ahora en la terminal.\n", "Si la terminal sigue colapsando, ejecuta en su lugar el siguiente comando:\n", "`terraform apply -no-color  2> errors.txt`\n", "Esto crear\u00e1 un archivo de texto que contiene el mensaje de error sin hacer que la terminal colapse.\n"]}, {"cell_type": "markdown", "id": "41cf284f-40a9-4f3d-b6ed-634feffd029b", "metadata": {}, "source": ["4.5. Inicializa la configuraci\u00f3n de Terraform con los proveedores requeridos:\n", "\n", "``` bash\n", "terraform init\n", "```\n", "\n", "Deber\u00edas obtener una salida similar a esta:\n", "\n", "![terraform_init](./images/terraform_init.png)\n"]}, {"cell_type": "markdown", "id": "e73e522e-9210-4fb6-a762-f95da137f9da", "metadata": {}, "source": ["4.6. Generar el plan de Terraform que te permite previsualizar los cambios en la infraestructura antes de aplicarlos. Cuando ejecutas el comando correspondiente, Terraform analiza los archivos de configuraci\u00f3n para verificar el estado deseado, compara el estado deseado con el estado actual de la infraestructura y calcula las acciones necesarias para lograrlo. Ejecuta el siguiente comando para generar el plan:\n", "\n", "\n", "``` bash\n", "terraform plan\n", "```\n", "\n", "Obtendr\u00e1s un resumen de los cambios planificados que indican los recursos que se agregar\u00e1n, modificar\u00e1n o eliminar\u00e1n. Si falla al ejecutarse, verifica que hayas asignado correctamente las variables de entrada en el paso anterior.\n"]}, {"cell_type": "markdown", "id": "321fc20a-144e-40a8-924e-cf6bb4046a7d", "metadata": {}, "source": ["4.7. Ir a la pesta\u00f1a previamente abierta de la consola de AWS en tu navegador, buscar *Glue* y abrir *AWS Glue*. Ingresa *trabajos ETL* en el lado izquierdo del panel de control. Deber\u00edas ver que a\u00fan no hay trabajos definidos.\n"]}, {"cell_type": "markdown", "id": "6ba748dc-b5f3-42d0-bf8e-49b9e6be0298", "metadata": {}, "source": ["4.8. Aplica la configuraci\u00f3n ejecutando:\n", "\n", "``` bash\n", "terraform apply\n", "```\n", "\n", "Te mostrar\u00e1 el mismo plan generado anteriormente y te pedir\u00e1 que respondas `yes`:\n", "\n", "![terraform_plan](images/terraform_plan.png)\n"]}, {"cell_type": "markdown", "id": "9447b5ac-d41a-4c5f-97a2-c9f91d0fb50d", "metadata": {}, "source": ["4.9. Verifique el progreso de la implementaci\u00f3n a trav\u00e9s de la terminal.\n", "Terraform crear\u00e1 algunos recursos en su cuenta de AWS. Una vez que termine, recibir\u00e1 una confirmaci\u00f3n similar a esta:\n", "\n", "![stack_deployed](images/stack_deployed.png)\n"]}, {"cell_type": "markdown", "id": "b354a4f1-43e2-419d-812f-3cc9b69027b2", "metadata": {}, "source": ["4.10. Regresa al panel de control de AWS Glue e ingresa en el enlace de *trabajos ETL*.\n", "Ahora ver\u00e1s un trabajo creado con el nombre `de-c1w2-etl-job` que acabas de crear con IaC.\n"]}, {"cell_type": "markdown", "id": "e97e80e9-a8bb-48da-9342-695ba100dda1", "metadata": {}, "source": ["4.11. Iniciar el trabajo de AWS Glue:\n", "\n", "``` bash\n", "aws glue start-job-run --job-name de-c1w2-etl-job | jq -r '.JobRunId'\n", "```\n", "\n", "Deber\u00edas obtener `JobRunID` en la salida.\n"]}, {"cell_type": "markdown", "id": "d8e16d58-b593-41db-95d4-69e545cbadc5", "metadata": {}, "source": ["4.12. Verifique el estado del trabajo de AWS Glue intercambiando `<JobRunID>` con la salida del paso anterior:\n", "\n", "``` bash\n", "aws glue get-job-run --job-name de-c1w2-etl-job --run-id <JobRunID> --output text --query \"JobRun.JobRunState\"\n", "```\n", "\n", "Tambi\u00e9n puede ver el estado del trabajo en la consola abriendo el `de-c1w2-etl-job` y yendo a la pesta\u00f1a `Runs`. Espere hasta que el estado del trabajo cambie a `SUCCEEDED` (esto tomar\u00e1 de 2 a 3 minutos).\n", "\n", "\n", "El trabajo de Glue transform\u00f3 los datos, pas\u00e1ndolos de un \n", "[esquema original](https://www.mysqltutorial.org/mysql-sample-database.aspx) a otro:\n", "\n", "![schema_after_ETL.png](images/schema_after_ETL.png)\n", "\n", "Esta nueva forma de datos se conoce como esquema estrella. Consiste en tablas de hechos y dimensiones. La tabla fact_orders representa las mediciones que resultaron de un evento comercial. En este ejemplo, cada fila en la tabla de hechos corresponde a un pedido de venta y contiene mediciones relacionadas como la cantidad pedida y el precio. Las tablas de dimensiones proporcionan m\u00e1s contexto para las mediciones de la tabla de hechos. En este ejemplo, las tablas de dimensiones proporcionan m\u00e1s informaci\u00f3n relacionada con los clientes, las ubicaciones de los clientes y los detalles de los pedidos. \u00bfC\u00f3mo facilita esta forma el trabajo para los analistas de datos? La tabla de hechos contiene las mediciones que necesitan agregar (total de ventas, promedio de precios, ...) y las tablas de dimensiones ayudan a hacer estas agregaciones m\u00e1s espec\u00edficas (total de ventas realizadas en un pa\u00eds dado, n\u00famero m\u00e1ximo de cantidades pedidas para cada l\u00ednea de productos). Esto se puede hacer escribiendo declaraciones de consulta simples, que podr\u00edan haber sido m\u00e1s complicadas si se hubieran realizado en la tabla original. Aprender\u00e1 m\u00e1s sobre modelado de datos en el Curso 4 de esta especializaci\u00f3n.\n"]}, {"cell_type": "markdown", "id": "1b44f1e1-f12d-4318-84e4-81ecc49d82b9", "metadata": {}, "source": ["<div id='5'/>\n", "\n", "## 5 - Datos e informaci\u00f3n visual\n", "\n", "Tu rol como Ingeniero de Datos podr\u00eda terminar aqu\u00ed. Todo lo que necesitas hacer ahora es darle acceso al analista de datos a los datos almacenados en S3 para que puedan consultarlos utilizando Amazon Athena. Para este laboratorio, tambi\u00e9n experimentar\u00e1s con este \u00faltimo paso que es consultar los datos utilizando Amazon Athena y crear algunas visualizaciones para analizar los datos. Llevar\u00e1s a cabo el an\u00e1lisis en Jupyter Notebook.\n"]}, {"cell_type": "markdown", "id": "e9bad255-69ea-423c-a70b-6a86a267ae3b", "metadata": {}, "source": ["5.1. Abre el cuaderno *C1_W2_Dashboard.ipynb* y sigue las instrucciones all\u00ed.\n", "\n", "Despu\u00e9s de seguir las instrucciones del archivo del cuaderno, env\u00eda tu laboratorio haciendo clic en **Enviar tarea** (esquina superior derecha).\n"]}, {"cell_type": "code", "execution_count": null, "id": "90086049-d7f8-4d84-b00e-39aef2883bb1", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "ce2252e6-3f13-4682-9add-2052e5498893", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "9fad8d78-f7f4-47a2-b659-65ad6c286cb2", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.4"}}, "nbformat": 4, "nbformat_minor": 5}