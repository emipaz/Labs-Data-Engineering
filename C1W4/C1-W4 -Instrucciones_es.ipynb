{"cells": [{"cell_type": "markdown", "id": "6de92305-cbab-477d-8e29-65309621d36d", "metadata": {}, "source": ["# Tarea de la Semana 4:<br>Construyendo Pipelines de Lotes y de Streaming de principio a fin<br>Basado en los Requerimientos de las Partes Interesadas\n", "\n", "En este laboratorio, experimentar\u00e1s un paso m\u00e1s con el marco de \"Pensar como un Ingeniero de Datos\". Implementar\u00e1s las arquitecturas de lotes y de streaming que has explorado en los cuestionarios de esta semana para cumplir con los requerimientos de las partes interesadas. Primero comenzar\u00e1s implementando el pipeline de lotes que sirve los datos de entrenamiento para el sistema de recomendaci\u00f3n. Luego almacenar\u00e1s los embeddings de salida del modelo en una base de datos vectorial, y finalmente implementar\u00e1s el pipeline de streaming que utiliza el modelo entrenado y la base de datos vectorial para proporcionar recomendaciones de productos en tiempo real.\n", "\n", "# Tabla de Contenidos\n", "- [1 - Implementando el Pipeline de Lotes](#1)\n", "- [2 - Creando y Configurando la Base de Datos Vectorial](#2)\n", "- [3 - Conectando el Modelo Implementado a la Base de Datos Vectorial](#3)\n", "- [4 - Implementando el Pipeline de Streaming](#4)\n", "\n", "---\n"]}, {"cell_type": "markdown", "id": "4441fd9d-bb29-4e78-a7ef-fa0ac66e49a4", "metadata": {}, "source": ["<div id='1'/>\n", "\n", "## 1 - Implementaci\u00f3n del Pipeline por Lotes\n", "\n", "Aqu\u00ed est\u00e1 el diagrama arquitect\u00f3nico del pipeline por lotes:\n", "\n", "![batch_process](./images/de-c1w4-diagram-batch.drawio.png)\n", "\n", "El pipeline por lotes ingiere datos de productos y usuarios desde la base de datos fuente (base de datos MySQL de Amazon RDS), los transforma utilizando AWS Glue ETL y almacena los datos transformados en un bucket de Amazon S3. Los datos se transforman en datos de entrenamiento que ser\u00e1n utilizados por el cient\u00edfico de datos para entrenar el sistema de recomendaci\u00f3n.\n", "\n", "Antes de implementar este pipeline, explorar\u00e1s una tabla adicional que se agreg\u00f3 a la base de datos de muestra de MySQL `classicmodels` utilizada en la Semana 2. La tabla adicional est\u00e1 etiquetada como `ratings` y consiste en las calificaciones asignadas por los usuarios a los productos que han comprado. Las calificaciones est\u00e1n en una escala del 1 al 5 y fueron generadas para este laboratorio.\n", "\n", "**Explorando la tabla `ratings`**\n", "\n", "La base de datos fuente (base de datos MySQL de Amazon RDS) ya est\u00e1 instanciada y provisionada para que la uses en este laboratorio. Con\u00e9ctate a ella y verifica la tabla `ratings`.\n"]}, {"cell_type": "markdown", "id": "7685ca21-824e-4e27-8d25-a882d82a246b", "metadata": {}, "source": ["1.1. Obtenga el punto final (es decir, la direcci\u00f3n) de la instancia de base de datos ejecutando el siguiente comando (reemplace `<MySQL-DB-name>` con `de-c1w4-rds`) en la terminal de VSCode. Si es necesario, abra una nueva terminal seleccionando Terminal > Nueva terminal en el men\u00fa.\n", "\n", "```bash\n", "aws rds describe-db-instances --db-instance-identifier <MySQL-DB-name> --output text --query \"DBInstances[].Endpoint.Address\"\n", "```\n"]}, {"cell_type": "markdown", "id": "09580aba-66ba-4b3a-a723-ba01ddfc018f", "metadata": {}, "source": ["1.2. Con\u00e9ctese a la base de datos ejecutando el siguiente comando, reemplazando\n", "   - `<MySQLEndpoint>` con la salida del paso anterior,\n", "\n", "   - `<DatabaseUserName>` con `admin` ,\n", "\n", "   - `<Password>` con `adminpwrd` :\n", "\n", "```bash\n", "mysql --host=<MySQLEndpoint> --user=<DatabaseUserName> --password=<Password> --port=3306\n", "```\n"]}, {"cell_type": "markdown", "id": "1b22caa7-5a25-4bb3-8a65-e42c7d186867", "metadata": {}, "source": ["1.3. Aseg\u00farate de usar la base de datos `classicmodels` y listar las tablas existentes:\n", "\n", "```bash\n", "use classicmodels;\n", "show tables;\n", "```\n", "\n", "Deber\u00edas ver la nueva tabla: `ratings`.\n"]}, {"cell_type": "markdown", "id": "23d54eae-3fd4-42c4-b4da-49cc2dfd3990", "metadata": {}, "source": ["1.4. Para verificar las primeras 20 l\u00edneas de la tabla, ejecuta la siguiente consulta:\n", "\n", "```sql\n", "SELECT * \n", "FROM ratings\n", "LIMIT 20;\n", "```\n", "\n", "La cl\u00e1usula `LIMIT` limita la cantidad de filas mostradas en el resultado de la consulta, \n", "puedes ajustar este n\u00famero para ver m\u00e1s o menos datos.\n", "    \n", "Intenta comprender la estructura de la tabla `ratings` y d\u00f3nde deber\u00eda colocarse en el esquema original \n", "[esquema](https://www.mysqltutorial.org/getting-started-with-mysql/mysql-sample-database/).\n"]}, {"cell_type": "markdown", "id": "576c3f75-7e0c-47c9-92e6-00db0d83185a", "metadata": {}, "source": ["1.5. Cerrar la conexi\u00f3n a la base de datos:\n", "\n", "```bash\n", "exit\n", "```\n", "\n", "**Ejecuci\u00f3n del trabajo de ETL de AWS Glue**\n", "\n", "Ahora crear\u00e1 los recursos necesarios para el pipeline por lotes (AWS Glue ETL y\n", "dep\u00f3sito de Amazon S3) utilizando [Terraform](https://www.terraform.io/). Recuerde que Terraform es una herramienta de Infraestructura como C\u00f3digo (IaC)\n", "que le permite configurar y aprovisionar recursos para su flujo de trabajo.\n", "Una vez que haya creado los recursos, ejecutar\u00e1 el trabajo de AWS Glue que ingesta\n", "los datos de RDS y los transforma en los datos de entrenamiento solicitados por el cient\u00edfico de datos.\n"]}, {"cell_type": "markdown", "id": "1e96895e-10e0-4bf9-a240-3c7197a9d7a6", "metadata": {}, "source": ["1.6. Configurar el entorno ejecutando el script `scripts/setup.sh`:\n", "\n", "```bash\n", "source ./scripts/setup.sh\n", "```\n", "\n", "Este script configura algunas variables de entorno necesarias para pasar par\u00e1metros a la configuraci\u00f3n de Terraform.\n"]}, {"cell_type": "markdown", "id": "5e8cacfa-26bd-4f6d-a644-c6d01f55d477", "metadata": {}, "source": ["1.7. Ir a la carpeta `terraform`.\n", "\n", "```bash\n", "cd terraform\n", "```\n"]}, {"cell_type": "markdown", "id": "3748cd79-78e4-48af-96e6-b139a1804493", "metadata": {}, "source": ["1.8. Abre el script de Terraform `terraform/main.tf`. Para la parte del pipeline por lotes, solo tienes que trabajar en `module \"etl\"`. Descomenta la secci\u00f3n correspondiente del archivo (l\u00edneas 1 a 15), manteniendo el resto del archivo comentado (los comentarios en Terraform comienzan con `#`).\n", "\n", "*Nota*: Recuerda guardar tus cambios presionando `Ctrl+S` o `Cmd+S`.\n"]}, {"cell_type": "markdown", "id": "69ce0661-17c6-476e-85cc-85d26f018a40", "metadata": {}, "source": ["1.9. Abre el script `terraform/outputs.tf` y descomenta las l\u00edneas correspondientes a la secci\u00f3n `ETL` (l\u00edneas 2 a 8).\n", "\n", "*Nota*: Recuerda guardar tus cambios presionando `Ctrl+S` o `Cmd+S`.\n"]}, {"cell_type": "markdown", "id": "31887263-b41f-4d17-af2d-cd46bf9ac68b", "metadata": {}, "source": ["1.10. Inicializa la configuraci\u00f3n de Terraform:\n", "\n", "```bash\n", "terraform init\n", "```\n", "\n", "1.11. Para implementar los recursos, ejecuta los siguientes comandos:\n", "\n", "```bash\n", "terraform plan\n", "terraform apply\n", "```\n", "\n", "*Nota*: El comando `terraform apply` te pedir\u00e1 que respondas `yes` aqu\u00ed:\n", "\n", "![terraform_plan](images/terraform_plan.png)\n", "\n", "*Nota*: Si hay errores en los comandos o en los archivos de configuraci\u00f3n de Terraform, la terminal puede colapsar. \n", "Cuando esto suceda, ver\u00e1s el siguiente mensaje:\n", "\n", "![etl_diagram](images/terminal_crash.png)\n", "\n", "Puedes volver a abrir la terminal presionando <code>Ctrl + \\`</code> (o <code>Cmd + \\`</code>) o navegando a Ver > Terminal. \n", "En la terminal, vuelve a la carpeta de Terraform (`cd terraform`) y luego intenta \n", "ejecutar nuevamente los comandos requeridos. El error deber\u00eda aparecer ahora en la terminal.\n", "Si la terminal sigue colapsando, ejecuta en su lugar el siguiente comando:\n", "`terraform apply -no-color  2> errors.txt`\n", "Esto crear\u00e1 un archivo de texto que contiene el mensaje de error sin hacer que la terminal colapse.\n"]}, {"cell_type": "markdown", "id": "5f9c78ee-477a-4ade-8200-28d13acd99fb", "metadata": {}, "source": ["1.12. Para acceder a la consola de AWS, ejecuta el siguiente comando en la terminal.\n", "\n", "```bash\n", "cat ~/.aws/aws_console_url\n", "```\n", "Abre el enlace en una nueva ventana del navegador.\n", "\n", "*Nota*: Por razones de seguridad, la URL para acceder a la consola de AWS caducar\u00e1 cada 15 minutos, \n", "pero cualquier recurso de AWS que hayas creado seguir\u00e1 estando disponible durante un per\u00edodo de 2 horas. \n", "Si necesitas acceder a la consola despu\u00e9s de 15 minutos, vuelve a ejecutar el comando para obtener un nuevo enlace activo.\n", "\n", "*Nota:* Si ves la ventana como en la siguiente captura de pantalla, haz clic en el enlace **logout**, \n", "cierra la ventana y vuelve a hacer clic en el enlace de la consola.\n", "\n", "![AWSLogout](images/AWSLogout.png)\n"]}, {"cell_type": "markdown", "id": "a669f1ed-6be5-497d-b6ae-a7f4fedc7920", "metadata": {}, "source": ["1.13. En la consola de AWS, busca **AWS Glue** e ingresa al enlace de **trabajos ETL** en el panel izquierdo. Deber\u00edas ver un trabajo creado con el nombre `de-c1w4-etl-job`. Para iniciar el trabajo de AWS Glue, ejecuta el siguiente comando:\n", "\n", "```bash\n", "aws glue start-job-run --job-name de-c1w4-etl-job | jq -r '.JobRunId'\n", "```\n", "\n", "Deber\u00edas obtener `JobRunID` en la salida.\n"]}, {"cell_type": "markdown", "id": "af7f27d9-b5e8-4844-bf6e-6b52a9f10ecc", "metadata": {}, "source": ["1.14. Verifique el estado del trabajo de AWS Glue intercambiando `<JobRunID>` con la salida del paso anterior:\n", "\n", "```bash\n", "aws glue get-job-run --job-name de-c1w4-etl-job --run-id <JobRunID> --output text --query \"JobRun.JobRunState\"\n", "```\n", "\n", "Tambi\u00e9n puede ver el estado del trabajo en la consola abriendo `de-c1w4-etl-job` y yendo a la pesta\u00f1a `Runs`. Espere hasta que el estado del trabajo cambie a `SUCCEEDED` (tomar\u00e1 2-3 minutos).\n", "\n", "Los datos transformados tienen el siguiente esquema:\n", "\n", "![schema_after_ETL](./images/schema_after_ETL.png)\n"]}, {"cell_type": "markdown", "id": "aa9b517a-ae93-4cca-b262-d3911cef97e2", "metadata": {}, "source": ["1.15. El trabajo de AWS Glue debe transformar los datos y almacenarlos en el bucket del lago de datos S3 que se cre\u00f3 con IaC. Puedes ir a la consola de AWS, buscar **S3** y luego buscar un bucket llamado `de-c1w4-<PLACEHOLDER>-datalake`, donde `<PLACEHOLDER>` es el n\u00famero de tu cuenta de AWS. Bajo la carpeta `ratings-ml-training` puedes ver algunas carpetas adicionales con la convenci\u00f3n de nomenclatura: `customerNumber=<NUMBER>`. Esta convenci\u00f3n indica c\u00f3mo se particionaron los datos durante el paso de almacenamiento. El concepto de particionamiento se abordar\u00e1 m\u00e1s adelante en la especializaci\u00f3n.\n", "\n", "Una vez que los datos est\u00e9n all\u00ed, el equipo de ML tomar\u00e1 estos datos y entrenar\u00e1 su modelo.\n"]}, {"cell_type": "markdown", "id": "014a981c-2ee4-4cc3-8fc7-033a2bd92872", "metadata": {}, "source": ["<div id='2'/>\n", "\n", "## 2 - Creaci\u00f3n y configuraci\u00f3n de la base de datos de vectores\n", "\n", "2.1. Ahora, supongamos que el Cient\u00edfico de Datos ha entrenado el modelo. Crearon el bucket S3\n", "llamado `de-c1w4-<AWS-ACCOUNT-ID>-us-east-1-ml-artifacts`. En ese bucket, el Cient\u00edfico de Datos\n", "public\u00f3 los resultados del modelo. Puedes explorar el bucket y encontrar la siguiente\n", "estructura de carpetas:\n", "\n", "```bash\n", ".\n", "\u251c\u2500\u2500 embeddings/\n", "|   \u251c\u2500\u2500 item_embeddings.csv\n", "|   \u2514\u2500\u2500 user_embeddings.csv\n", "\u251c\u2500\u2500 model/\n", "|   \u2514\u2500\u2500 best_model.pth   \n", "\u2514\u2500\u2500 scalers/\n", "    \u251c\u2500\u2500 item_ohe.pkl\n", "    \u251c\u2500\u2500 item_std_scaler.pkl\n", "    \u251c\u2500\u2500 user_ohe.pkl\n", "    \u2514\u2500\u2500 user_std_scaler.pkl   \n", "```\n", "\n", "La carpeta `embeddings` contiene los embeddings de los usuarios y elementos (o productos) que\n", "fueron generados en el modelo.\n", "\n", "La carpeta `model` contiene el modelo entrenado que se utilizar\u00e1 para inferencia.\n", "\n", "La carpeta `scalers` contiene los objetos utilizados en la parte de preprocesamiento para\n", "el entrenamiento, como \n", "[One Hot Encoders](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) \n", "y [Standard Scalers](https://en.wikipedia.org/wiki/Feature_scaling).\n", "\n", "El Cient\u00edfico de Datos pidi\u00f3 subir los archivos `item_embeddings.csv` y `user_embeddings.csv`\n", "a una base de datos de vectores (Vector DB). Esos embeddings ser\u00e1n utilizados por el\n", "sistema de recomendaci\u00f3n en el pipeline de streaming para proporcionar recomendaciones de productos.\n", "El uso de una base de datos de vectores acelera la recuperaci\u00f3n de elementos que son similares a un\n", "elemento dado. Entonces, por ejemplo, si un usuario coloca un art\u00edculo en el carrito, el modelo de recomendaci\u00f3n\n", "primero calcular\u00e1 el vector de embedding de este art\u00edculo y luego usar\u00e1 la base de datos de vectores\n", "para recuperar elementos similares a \u00e9l.\n", "\n", "**Creaci\u00f3n de la base de datos de vectores**\n", "\n", "Para la base de datos de vectores, crear\u00e1s con Terraform una base de datos RDS PostgreSQL\n", "con la [extensi\u00f3n](https://github.com/pgvector/pgvector) `pgvector`. La base de datos PostgreSQL\n", "es t\u00edpicamente m\u00e1s r\u00e1pida para consultas complejas y an\u00e1lisis de datos, mientras que MySQL,\n", "que utilizaste anteriormente, es m\u00e1s eficiente para consultas m\u00e1s simples. Trabajar con Vector\n", "DB PostgreSQL proporciona m\u00e1s capacidades y flexibilidad.\n"]}, {"cell_type": "markdown", "id": "310a3ec8-cdf5-4ff7-ba1c-e1ee7b44608b", "metadata": {}, "source": ["2.2. En el explorador de VSCode, abre nuevamente el script `/terraform/main.tf`. Descomenta la secci\u00f3n `module \"vector_db\"` (l\u00edneas 17 a 27).\n", "\n", "*Nota*: Recuerda guardar tus cambios presionando `Ctrl+S` o `Cmd+S`.\n", "\n", "2.3. En el archivo `/terraform/outputs.tf`, descomenta los bloques asociados con la base de datos Vector (l\u00edneas 11 a 27).\n", "\n", "*Nota*: Recuerda guardar tus cambios presionando `Ctrl+S` o `Cmd+S`.\n", "\n", "2.4. Reinicializa el m\u00f3dulo de terraform, planifica y aplica los cambios con los siguientes comandos (aseg\u00farate de ejecutarlos desde la carpeta `terraform` en la terminal):\n", "\n", "```bash\n", "terraform init\n", "terraform plan\n", "terraform apply\n", "```\n", "\n", "*Nota*: El comando `terraform apply` te pedir\u00e1 que respondas `yes` aqu\u00ed:\n", "\n", "![terraform_plan](images/terraform_plan_2.png)\n", "\n", "*La creaci\u00f3n de la base de datos tomar\u00e1 alrededor de 7 minutos*.\n"]}, {"cell_type": "markdown", "id": "9e2af2d4-e9e9-478e-8e3e-e9a79598e399", "metadata": {}, "source": ["2.5. Ver\u00e1s que se muestra cierta informaci\u00f3n despu\u00e9s del comando `terraform apply`, pero algunos campos (nombre de usuario y contrase\u00f1a de la base de datos) aparecen como sensibles. Para ver esa informaci\u00f3n, que se utilizar\u00e1 para conectarse a la base de datos Vector, necesitas usar los siguientes comandos:\n", "\n", "```bash\n", "terraform output vector_db_master_username\n", "terraform output vector_db_master_password\n", "```\n", "\n", "Guarda las salidas en alg\u00fan lugar localmente, las usar\u00e1s m\u00e1s tarde.\n", "\n", "*Nota*: Las salidas se imprimen entre comillas dobles, las cuales no forman parte del nombre de usuario o la contrase\u00f1a.\n", "\n", "**Agregando los embeddings a la base de datos vector**\n", "\n", "Ahora que se ha creado la base de datos vector, te conectar\u00e1s a ella para importar los embeddings desde S3. El archivo `sql/embeddings.sql` contiene las declaraciones SQL para esta tarea.\n"]}, {"cell_type": "markdown", "id": "ce06e89f-6a70-4d14-97af-965adfa06105", "metadata": {}, "source": ["2.6. Abre el archivo `sql/embeddings.sql` y cambia los marcadores de posici\u00f3n del bucket `<BUCKET_NAME>` con el nombre del bucket `de-c1w4-<AWS-ACCOUNT-ID>-us-east-1-ml-artifacts` (donde `<AWS-ACCOUNT-ID>` es tu ID de cuenta de AWS). Puedes ir a la consola de AWS, buscar S3 y luego buscar un bucket llamado `de-c1w4-<AWS-ACCOUNT-ID>-us-east-1-ml-artifacts`. Otra opci\u00f3n es ejecutar el comando `aws s3 ls` en la terminal.\n", "\n", "Puedes regresar al paso 2.1 para obtener el nombre del bucket que contiene los embeddings.\n", "\n", "*Nota*: \u00a1El cambio de los marcadores de posici\u00f3n del bucket `<BUCKET_NAME>` debe hacerse en dos lugares!\n", "\n", "*Nota*: Recuerda guardar tus cambios presionando `Ctrl+S` o `Cmd+S`.\n"]}, {"cell_type": "markdown", "id": "1337a968-d21c-4321-a12e-f31b2dbf87ee", "metadata": {}, "source": ["2.7. Obt\u00e9n la salida de `vector_db_host` de Terraform. Puedes ver la salida que se devolvi\u00f3 al final del paso 2.4 o ejecutar el comando `terraform output vector_db_host`.\n", "\n", "M\u00e1s adelante usar\u00e1s esta salida, as\u00ed que gu\u00e1rdala en alg\u00fan lugar local.\n", "\n", "2.8. Ahora conecta a la base de datos ejecutando el siguiente comando, reemplazando `<VectorDBHost>` con la salida del paso anterior. Se te pedir\u00e1 una contrase\u00f1a, puedes usar la obtenida en el paso 2.5.\n", "\n", "```bash\n", "psql --host=<VectorDBHost> --username=postgres --password --port=5432\n", "```\n", "\n", "2.9. Luego, para trabajar en la base de datos de postgres usa este comando con la misma contrase\u00f1a:\n", "\n", "```bash\n", "\\c postgres;\n", "```\n", "\n", "2.10. Una vez conectado a la base de datos de postgres, ahora puedes ejecutar las declaraciones SQL en el script `sql/embeddings.sql`. Usa el siguiente comando:\n", "\n", "```bash\n", "\\i '../sql/embeddings.sql'\n", "```\n"]}, {"cell_type": "markdown", "id": "9f33ff8a-f60c-46fb-ac79-ae4ba0f96367", "metadata": {}, "source": ["2.11. Para verificar las tablas disponibles, usa el siguiente comando:\n", "\n", "```bash\n", "\\dt *.*\n", "```\n", "Presiona la tecla `Q` para salir del prompt de `psql`.\n", "\n", "2.12. Sal del prompt de `psql` con el comando `\\q` (o `exit`).\n", "\n", "*Opcional*: Si est\u00e1s interesado en aprender m\u00e1s sobre las banderas de postgres, puedes consultar [link](https://hasura.io/blog/top-psql-commands-and-flags-you-need-to-know-postgresql/).\n"]}, {"cell_type": "markdown", "id": "5a7e4aa2-7808-4f56-aa1f-c89b52bbfadf", "metadata": {}, "source": ["<div id='3'/>\n", "\n", "## 3 - Conectando el Modelo Implementado a la Base de Datos de Vectores\n", "\n", "Ahora que tienes la base de datos de vectores creada y los embeddings importados en ella. Veamos d\u00f3nde encaja en el flujo de trabajo de transmisi\u00f3n. Aqu\u00ed est\u00e1 el diagrama arquitect\u00f3nico del flujo de trabajo de transmisi\u00f3n:\n", "\n", "![stream_process](./images/de-c1w4-diagram-stream.drawio.png)\n", "\n", "En el lado izquierdo, ves una funci\u00f3n lambda etiquetada como \"inferencia de modelo\". Esta funci\u00f3n lambda, que es un recurso inform\u00e1tico sin servidor, utilizar\u00e1 el modelo entrenado almacenado en S3 y los embeddings de la base de datos de vectores para proporcionar la recomendaci\u00f3n basada en la actividad en l\u00ednea del usuario transmitida por los flujos de datos de Kinesis.\n", "\n", "En esta secci\u00f3n, configurar\u00e1s las variables en la funci\u00f3n lambda para prepararla para el flujo de trabajo de transmisi\u00f3n.\n", "\n", "3.1. En la Consola de AWS, busca **RDS**, haz clic en **Bases de datos** en el panel izquierdo y encuentra la base de datos llamada `de-c1w4-vector-db`. Haz clic en ella.\n", "\n", "3.2. En la pesta\u00f1a *Conectividad y seguridad*, busca el endpoint, que deber\u00eda tener la siguiente estructura: `de-c1w4-vector-db.xxxxxxxxxxxx.us-east-1.rds.amazonaws.com`. Copia este valor y gu\u00e1rdalo en un lugar seguro, ya que lo usar\u00e1s en un momento (este es el mismo endpoint que obtuviste con el comando `terraform output vector_db_host`).\n", "\n", "3.3. De regreso a la Consola de AWS, busca *Lambda*. Luego encuentra la funci\u00f3n lambda `de-c1w4-model-inference`. Haz clic en el nombre de la lambda y luego despl\u00e1zate hacia abajo y busca la pesta\u00f1a `Configuraci\u00f3n`, haz clic en ella. Luego, abre Variables de entorno desde el panel izquierdo y haz clic en `Editar`.\n", "\n", "Coloca los valores para las siguientes variables:\n", "   - `VECTOR_DB_HOST`:  Endpoint de VectorDB que copiaste en el paso anterior,\n", "\n", "   - `VECTOR_DB_PASSWORD`: valor de salida del comando `terraform output vector_db_master_password`,\n", "   \n", "   - `VECTOR_DB_USER`: valor de salida del comando `terraform output vector_db_master_username`.\n", "\n", "Haz clic en `Guardar`.\n"]}, {"cell_type": "markdown", "id": "ab6bf425-0988-47bb-8cca-2204ae5b3d23", "metadata": {}, "source": ["## 4 - Implementando el Pipeline de Streaming\n", "\n", "Ahora implementar\u00e1s el pipeline de streaming que consta de Kinesis Data Streams, Kinesis Firehose y S3 (bucket de recomendaciones). Supongamos que AWS Kinesis Data Streams recibe la actividad en l\u00ednea de los usuarios desde el registro de la plataforma de ventas. Luego transmite estos datos a Kinesis Data Firehose, que act\u00faa como un servicio de entrega que se encarga de cargar los flujos de datos en S3 (bucket de recomendaciones). Antes de cargar los datos en S3, invoca la funci\u00f3n lambda (transformaci\u00f3n de streaming) que extrae las caracter\u00edsticas de usuario y producto de los flujos de datos y utiliza el modelo recomendador entrenado para encontrar las recomendaciones y finalmente carga las recomendaciones en el bucket de recomendaciones de S3.\n", "\n", "Ahora usa Terraform para crear Kinesis Firehose, el bucket de recomendaciones de S3 y la funci\u00f3n lambda (transformaci\u00f3n de streaming).\n"]}, {"cell_type": "markdown", "id": "f9040698-8d9f-4645-98dd-0787a5e49248", "metadata": {}, "source": ["4.1. En el explorador de VSCode, abre nuevamente el script `/terraform/main.tf`, descomenta la secci\u00f3n `module \"streaming_inference\"` (l\u00edneas 29 a 39).\n", "\n", "*Nota*: Recuerda guardar tus cambios presionando `Ctrl+S` o `Cmd+S`.\n", "\n", "4.2. En el archivo `/terraform/outputs.tf`, descomenta los bloques asociados con la inferencia de streaming (l\u00edneas 30 a 32).\n", "\n", "*Nota*: Recuerda guardar tus cambios presionando `Ctrl+S` o `Cmd+S`.\n", "\n", "4.3. Reinicializa el m\u00f3dulo de terraform, planifica y aplica los cambios (aseg\u00farate de ejecutar los comandos desde la carpeta `terraform` en la terminal):\n", "\n", "```bash\n", "terraform init\n", "terraform plan\n", "terraform apply\n", "```\n", "\n", "*Nota*: El comando `terraform apply` te pedir\u00e1 que respondas `yes`.\n"]}, {"cell_type": "markdown", "id": "bae7bbf3-5353-4a60-9fea-28a843486373", "metadata": {}, "source": ["4.4.  Ahora, de regreso en la Consola de AWS, busca **S3** y haz clic en el bucket de recomendaciones que se acaba de crear usando Terraform, llamado `de-c1w4-<PLACEHOLDER>-recommendations`. Despu\u00e9s de un tiempo, el flujo de entrega de Kinesis que creaste con Firehose comenzar\u00e1 a consumir datos del flujo de datos de Kinesis `de-c1w4-kinesis-data-stream`. Luego, realizar\u00e1 las transformaciones que configuraste en la funci\u00f3n Lambda y el bucket de S3 se llenar\u00e1 con algunos archivos. Tambi\u00e9n puedes ir a Lambda y buscar `de-c1w4-transformation-lambda` que fue creado con Terraform. Haz clic en \u00e9l y busca la pesta\u00f1a `Monitor`. Haz clic en `Ver registros de CloudWatch`.\n", "\n", "*Nota*: Puede que veas un mensaje de error en la parte superior de la p\u00e1gina *El grupo de registros no existe.* *El grupo de registros espec\u00edfico: /aws/lambda/de-c1w4-transformation-lambda no existe en esta cuenta o regi\u00f3n.* Si esperas un minuto aproximadamente y luego actualizas, ese mensaje de error desaparecer\u00e1 y deber\u00edas comenzar a ver las actividades de registro.\n", "\n", "All\u00ed podr\u00e1s ver los registros de la funci\u00f3n mientras realiza algunas transformaciones. A medida que `de-c1w4-kinesis-data-stream` recibe datos din\u00e1micamente (con un tiempo promedio entre eventos de 10 segundos), el bucket de S3 o los registros de Lambda pueden tardar algunos minutos en comenzar a mostrar datos o eventos. Puedes esperar alrededor de 5 minutos y actualizar la p\u00e1gina de S3/CloudWatch Logs para ver los datos entrantes.\n", "En el bucket de S3, los datos se particionar\u00e1n por fecha, por lo que ver\u00e1s una jerarqu\u00eda de directorios similar a la siguiente:\n", "\n", "```bash\n", ".\n", "\u251c\u2500\u2500 a\u00f1o/\n", "    \u2514\u2500\u2500 mes/\n", "        \u2514\u2500\u2500 d\u00eda/\n", "            \u2514\u2500\u2500 hora/\n", "                \u2514\u2500\u2500 de-c1w4-delivery-stream-<PLACEHOLDER>\n", "```\n", "\n", "En este laboratorio, pasaste por un pipeline de extremo a extremo tanto para casos de lotes como de streaming. Construiste un pipeline de datos por lotes con AWS Glue y S3 para proporcionar los datos de entrenamiento para el sistema de recomendaci\u00f3n, luego creaste una base de datos de vectores para almacenar los embeddings y finalmente construiste un pipeline de streaming en tiempo real utilizando AWS Kinesis Data Streams y Firehose. Y con eso, has visto c\u00f3mo traducir las necesidades de las partes interesadas en requisitos funcionales y no funcionales, elegir las herramientas apropiadas y luego construir el pipeline de datos.\n"]}, {"cell_type": "code", "execution_count": null, "id": "3cd1249f-93a3-4a14-8db1-66e33c97b41b", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.4"}}, "nbformat": 4, "nbformat_minor": 5}